[kafka_remote]
bootstrap.servers=kafka_remote_01:9094,kafka_remote_02:9194,kafka_remote_03:9294
security.protocol = SASL_PLAINTEXT
sasl.mechanisms = PLAIN
sasl.username = kafka_remote_username
sasl.password = kafka_remote_pwd

[kafka_main]
bootstrap.servers = localhost:9094,localhost:9194,localhost:9294
security.protocol = SASL_PLAINTEXT
sasl.mechanisms = PLAIN
sasl.username = kafka_local_username
sasl.password = kafka_local_pwd

[mongodb]
connection.string = mongodb://mongousername:mongopwd@localhost:27017/
database.name = project-1
collection.name = event-collector

# DETAILED PRODUCER CONFIGURATION
[producer_config]
acks = all
retries = 3
batch.size = 16384
queue.buffering.max.kbytes = 32768
linger.ms = 5

# DETAILED CONSUMER CONFIGURATION
[consumer_config]
group.id = collect-group-2
enable.auto.commit = false
heartbeat.interval.ms = 3000
auto.offset.reset = latest
# session.timeout.ms: The maximum time (in ms) the broker will wait for a
# heartbeat before considering the consumer dead and triggering a rebalance.
# Usually set to 3x the heartbeat.interval.ms.
session.timeout.ms = 10000

[remote_consumer_config]
group.id = tungcao-group-2
enable.auto.commit = false
heartbeat.interval.ms = 3000
auto.offset.reset = earliest
session.timeout.ms = 10000

[other]
remote.max.poll.records = 500
remote.poll.timeout = 5
source.max.poll.records = 500
source.poll.timeout = 1
source.topic.name = collect_events
source.dlq.topic.name = collect_events.dlq
remote.topic.name = product_view
